---
title: Сенсорные технологии в Android
ms.prod: xamarin
ms.assetid: 405A1FA0-4EFA-4AEB-B672-F36307B9CF16
ms.technology: xamarin-android
author: conceptdev
ms.author: crdun
ms.date: 03/01/2018
ms.openlocfilehash: 274c441e0507f100697fc153a9f748de1bce4cf3
ms.sourcegitcommit: 6264fb540ca1f131328707e295e7259cb10f95fb
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/16/2019
ms.locfileid: "69526072"
---
# <a name="touch-in-android"></a>Сенсорные технологии в Android

Подобно iOS, Android создает объект, содержащий данные о физическом взаимодействии пользователя с экраном &ndash; `Android.View.MotionEvent` объекта. Этот объект содержит данные, такие как выполняемые действия, место касания, степень применения нажима и т. д. `MotionEvent` Объект разбивает перемещение на следующие значения:

- Код действия, описывающий тип движения, например начальное касание, сенсорный переход по экрану или сенсорный ввод.

- Набор значений осей, описывающих позицию `MotionEvent` и другие свойства перемещения, например место касания, время касания и объем использованной нагрузки.
   Значения осей могут отличаться в зависимости от устройства, поэтому в предыдущем списке не описываются все значения осей.


`MotionEvent` Объект будет передан в соответствующий метод в приложении. Существует три способа реагирования на событие касания приложением Xamarin. Android:

- *Назначьте обработчик `View.Touch`*  событий`Android.Views.View` классу`EventHandler<View.TouchEventArgs>` , в котором приложения могут назначать обработчик. Это типичное поведение .NET.

- *Реализация`View.IOnTouchListener`* -экземпляры этого интерфейса могут быть назначены объекту представления с помощью представления. `SetOnListener`Method. Это функционально эквивалентно назначению обработчика событий для `View.Touch` события. Если существует общая или общая логика, которой может потребоваться много различных представлений, когда они затронуты, то будет более эффективным создание класса и реализация этого метода, чем Присвоение каждому представлению собственного обработчика событий.

- *Переопределение `View.OnTouchEvent`*  — все представления в подклассе `Android.Views.View`Android. Когда представление затронуто, Android вызывает метод `OnTouchEvent` и передает `MotionEvent` ему объект в качестве параметра.


> [!NOTE]
> Не все устройства Android поддерживают сенсорные экраны. 

Добавление следующего тега в файл манифеста приведет к тому, что Google Play будет отображать приложение только для тех устройств, для которых включено касание:

```xml
<uses-configuration android:reqTouchScreen="finger" />
```

## <a name="gestures"></a>Жесты

Жест — это рисуемая вручную фигура на сенсорном экране. С жестом может быть один или несколько штрихов, каждый штрих, состоящий из последовательности точек, созданных в другой точке контакта с экраном. Android поддерживает множество различных типов жестов, от простых вставляет по экрану до сложных жестов, затрагивающих несколько касаний.

Android предоставляет `Android.Gestures` пространство имен специально для управления жестами и реагирования на них. В сердце всех жестов есть специальный класс с именем `Android.Gestures.GestureDetector`. Как следует из названия, этот класс будет прослушивать жесты и события на основе `MotionEvents` предоставленной операционной системой.

Чтобы реализовать средство обнаружения жестов, действием необходимо создать `GestureDetector` экземпляр класса и предоставить `IOnGestureListener`копию, как показано в следующем фрагменте кода:

```csharp
GestureOverlayView.IOnGestureListener myListener = new MyGestureListener();
_gestureDetector = new GestureDetector(this, myListener);
```

Действие также должно реализовывать Онтаучевент и передавать Мотионевент детектору жестов. В следующем фрагменте кода приведен пример.

```csharp
public override bool OnTouchEvent(MotionEvent e)
{
    // This method is in an Activity
    return _gestureDetector.OnTouchEvent(e);
}
```

Когда экземпляр `GestureDetector` определяет нужный жест, он уведомляет действие или приложение, вызывая событие или через обратный вызов, `GestureDetector.IOnGestureListener`предоставленный.
Этот интерфейс предоставляет шесть методов для различных жестов:

- Вызывается, когда касание происходит, но не освобождается.

- *Онфлинг* — вызывается при возникновении вставляет и предоставляет данные в начале и в конце сенсорного ввода, вызвавшего событие.

- *Онлонгпресс* — вызывается при длительной нажатии.

- *OnScroll* -вызывается при возникновении события Scroll.

- *Оншовпресс* — вызывается после завершения работы, а событие перемещения или вверх не было выполнено.

- *Онсинглетапуп* — вызывается при возникновении одного касания.


Во многих случаях приложения могут заинтересовать только подмножество жестов. В этом случае приложения должны расширять класс Жестуредетектор. Симплеонжестурелистенер и переопределять методы, соответствующие интересующим Вас событиям.

## <a name="custom-gestures"></a>Пользовательские жесты

Жесты — это отличный способ взаимодействия пользователей с приложением. Интерфейсы API, которые мы видели до сих пор, достаточно для простых жестов, но могут доказать немного обременительным для более сложных жестов. Для облегчения работы с более сложными жестами Android предоставляет еще один набор API в пространстве имен Android. жесты, который упрощает некоторые косвенные нагрузки, связанные с пользовательскими жестами.

### <a name="creating-custom-gestures"></a>Создание настраиваемых жестов

Начиная с версии Android 1,6, пакет SDK для Android поставляется с предварительно установленным приложением в эмуляторе, называемом "Построитель жестов". Это приложение позволяет разработчику создавать предварительно определенные жесты, которые могут быть внедрены в приложение. На следующем снимке экрана показан пример построителя жестов.

[![Снимок экрана: построитель жестов с примерами жестов](touch-in-android-images/image11.png)](touch-in-android-images/image11.png#lightbox)

Улучшенную версию этого приложения, называемую инструментом жестов, можно найти Google Play. Инструмент жеста очень похож на построитель жестов, за исключением того, что он позволяет тестировать жесты после их создания. На следующем снимке экрана показан построитель жестов:

[![Снимок экрана инструмента "жест" с примерами жестов](touch-in-android-images/image12.png)](touch-in-android-images/image12.png#lightbox)

Инструмент "жест" является более полезным для создания настраиваемых жестов, так как он позволяет тестировать жесты при их создании и легко доступен с помощью Google Play.

Инструмент "жест" позволяет создать жест путем рисования на экране и назначения имени. После создания жестов они сохраняются в двоичном файле на SD-карте устройства. Этот файл необходимо получить с устройства, а затем упаковать с приложением в папке/Ресаурцес/рав. Этот файл можно получить из эмулятора с помощью Android Debug Bridge. В следующем примере показано, как скопировать файл из хранилища Galaxy в каталог ресурсов приложения:

```shell
$ adb pull /storage/sdcard0/gestures <projectdirectory>/Resources/raw
```

После получения файла он должен быть упакован в приложение внутри каталога/Ресаурцес/рав. Самый простой способ использовать этот файл жеста — загрузить файл в Жестурелибрари, как показано в следующем фрагменте кода:

```csharp
GestureLibrary myGestures = GestureLibraries.FromRawResources(this, Resource.Raw.gestures);
if (!myGestures.Load())
{
    // The library didn't load, so close the activity.
    Finish();
}
```

### <a name="using-custom-gestures"></a>Использование пользовательских жестов

Чтобы распознать пользовательские жесты в действии, к его макету должен быть добавлен объект Android. жестов. Жестуреоверлай. В следующем фрагменте кода показано, как программным способом добавить Жестуреоверлайвиев к действию:

```csharp
GestureOverlayView gestureOverlayView = new GestureOverlayView(this);
gestureOverlayView.AddOnGesturePerformedListener(this);
SetContentView(gestureOverlayView);
```

В следующем фрагменте кода XML показано, как добавить Жестуреоверлайвиев декларативно:

```xml
<android.gesture.GestureOverlayView
    android:id="@+id/gestures"
    android:layout_width="match_parent "
    android:layout_height="match_parent" />
```

`GestureOverlayView` Содержит несколько событий, которые будут создаваться во время процесса рисования жеста. Самое интересное событие — `GesturePerformed`. Это событие возникает, когда пользователь завершает рисование своего жеста.

При возникновении этого события действие предложит `GestureLibrary` выполнить попытку и сопоставить жест, который пользователь имеет с одним из жестов, созданных инструментом жеста. `GestureLibrary`вернет список объектов прогноза.

Каждый объект прогноза содержит оценку и имя одного из жестов в `GestureLibrary`. Чем выше оценка, тем выше вероятность того, что жест, названный в прогнозе, соответствует жесту, нарисованному пользователем.
В целом, баллы ниже 1,0 считаются неплохими совпадениями.

В следующем коде показан пример сопоставления жеста:

```csharp
private void GestureOverlayViewOnGesturePerformed(object sender, GestureOverlayView.GesturePerformedEventArgs gesturePerformedEventArgs)
{
    // In this example _gestureLibrary was instantiated in OnCreate
    IEnumerable<Prediction> predictions = from p in _gestureLibrary.Recognize(gesturePerformedEventArgs.Gesture)
    orderby p.Score descending
    where p.Score > 1.0
    select p;
    Prediction prediction = predictions.FirstOrDefault();

    if (prediction == null)
    {
        Log.Debug(GetType().FullName, "Nothing matched the user's gesture.");
        return;
    }

    Toast.MakeText(this, prediction.Name, ToastLength.Short).Show();
}
```

После этого вы должны понимать, как использовать касания и жесты в приложении Xamarin. Android. Теперь давайте перейдем к пошаговому руководству и посмотрим все концепции в рабочем примере приложения.



## <a name="related-links"></a>Связанные ссылки

- [Запуск Android Touch (пример)](https://docs.microsoft.com/samples/xamarin/monodroid-samples/applicationfundamentals-touch-start)
- [Окончательное касание Android (пример)](https://docs.microsoft.com/samples/xamarin/monodroid-samples/applicationfundamentals-touch-final)
